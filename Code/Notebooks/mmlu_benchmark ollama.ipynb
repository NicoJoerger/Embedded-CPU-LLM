{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU Benchmark for OpenAI-Compatible APIs (Ollama, OpenAI, etc.)\n",
    "\n",
    "This notebook runs the MMLU (Massive Multitask Language Understanding) benchmark against any OpenAI-compatible API, including Ollama, OpenAI, and other compatible services.\n",
    "\n",
    "MMLU is a benchmark that tests knowledge across 57 subjects including mathematics, US history, computer science, law, and more.\n",
    "\n",
    "## Configuration\n",
    "- **API Endpoint**: Configure the base URL for your OpenAI-compatible API (e.g., Ollama, OpenAI)\n",
    "- **Model Name**: Specify the model to use (e.g., \"llama3.2:3b\" for Ollama, \"gpt-4\" for OpenAI)\n",
    "- **Number of Questions**: Select how many questions to test (per subject or total)\n",
    "- **Subjects**: Choose specific subjects or test all\n",
    "- **Multi-Server Support**: Load balance across multiple API instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests datasets pandas tqdm numpy scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "API Base URL: http://127.0.0.1:11434\n",
      "Model: llama3:8b-instruct-q4_K_M\n",
      "Number of servers: 1\n",
      "Questions per subject: ALL\n",
      "Timeout per question: 600 seconds (10.0 minutes)\n",
      "Results directory: c:\\Users\\user\\NextCloud\\nextcloud.nicojoerger.de\\Documents\\Schulen\\Studium\\Abschlussarbeit\\Git\\Embedded-CPU-LLM\\Code\\Notebooks\\results\n",
      "Checkpoints: Enabled (interval: 500 questions)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# API Configuration - Ollama (local)\n",
    "API_BASE_URL = \"http://127.0.0.1:11434\"  # Ollama default\n",
    "MODEL_NAME = \"llama3:8b-instruct-q4_K_M\"  # Model name (e.g., \"llama3.2:3b\")\n",
    "\n",
    "# Multiple Servers Support (optional - for load balancing across multiple Ollama instances)\n",
    "# Leave as single URL if you only have one server\n",
    "API_SERVERS = [\n",
    "    {\"base_url\": API_BASE_URL, \"model\": MODEL_NAME},\n",
    "    # Add more servers here if needed:\n",
    "    # {\"base_url\": \"http://192.168.1.100:11434\", \"model\": \"llama3.2:3b\"},\n",
    "]\n",
    "\n",
    "# Benchmark Configuration\n",
    "NUM_QUESTIONS_PER_SUBJECT = None  # Set to None to run ALL questions, or a number to limit per subject\n",
    "SELECTED_SUBJECTS = None  # List of subjects to test, or None for all subjects\n",
    "TIMEOUT_SECONDS = 600  # Timeout for each question (10 minutes)\n",
    "\n",
    "# Checkpoint Configuration\n",
    "ENABLE_CHECKPOINTS = True  # Enable checkpoint/resume functionality\n",
    "CHECKPOINT_INTERVAL = 500  # Save checkpoint after every N questions (reduced from 1 to prevent I/O overhead)\n",
    "\n",
    "# Results Directory\n",
    "RESULTS_DIR = Path(\"results\")  # Results will be saved in Notebooks/results/\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Generation Settings (Ollama options)\n",
    "GENERATION_SETTINGS = {\n",
    "    \"temperature\": 0.0,  # Lower temperature for more deterministic answers\n",
    "    \"num_predict\": 3,    # Limit tokens since we only need A/B/C/D\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"API Base URL: {API_BASE_URL}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Number of servers: {len(API_SERVERS)}\")\n",
    "print(f\"Questions per subject: {NUM_QUESTIONS_PER_SUBJECT if NUM_QUESTIONS_PER_SUBJECT else 'ALL'}\")\n",
    "print(f\"Timeout per question: {TIMEOUT_SECONDS} seconds ({TIMEOUT_SECONDS/60:.1f} minutes)\")\n",
    "print(f\"Results directory: {RESULTS_DIR.absolute()}\")\n",
    "print(f\"Checkpoints: {'Enabled' if ENABLE_CHECKPOINTS else 'Disabled'} (interval: {CHECKPOINT_INTERVAL} questions)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def check_api_health(server_config: Dict) -> Dict:\n",
    "    \"\"\"Check if the Ollama API is accessible by making a simple request.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{server_config['base_url']}/api/tags\",\n",
    "            timeout=10\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"model\": server_config['model'],\n",
    "            \"base_url\": server_config['base_url']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise ConnectionError(f\"Failed to connect to API: {e}\")\n",
    "\n",
    "\n",
    "def generate_response(server_config: Dict, prompt: str, timeout: int = TIMEOUT_SECONDS) -> str:\n",
    "    \"\"\"Send a prompt to the Ollama API and get the generated response.\n",
    "    \n",
    "    Format matches the working HTTP file (Chat mit erweiterten Optionen).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build the request payload with Ollama format (matching working HTTP file)\n",
    "        payload = {\n",
    "            \"model\": server_config['model'],\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"stream\": False,\n",
    "            \"options\": GENERATION_SETTINGS\n",
    "        }\n",
    "\n",
    "        # Send request to Ollama API\n",
    "        response = requests.post(\n",
    "            f\"{server_config['base_url']}/api/chat\",\n",
    "            json=payload,\n",
    "            timeout=timeout,\n",
    "            headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse Ollama-style response\n",
    "        result = response.json()\n",
    "\n",
    "        # Validate response structure\n",
    "        if 'message' not in result or 'content' not in result['message']:\n",
    "            raise RuntimeError(f\"Invalid response format: {result}\")\n",
    "\n",
    "        generated_text = result['message']['content']\n",
    "\n",
    "        return generated_text.strip()\n",
    "\n",
    "    except requests.Timeout:\n",
    "        raise TimeoutError(f\"Request timed out after {timeout} seconds\")\n",
    "    except requests.RequestException as e:\n",
    "        raise RuntimeError(f\"Request failed: {e}\")\n",
    "    except KeyError as e:\n",
    "        raise RuntimeError(f\"Response parsing failed - missing key {e}: {result if 'result' in locals() else 'No response'}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Generation failed: {e}\")\n",
    "\n",
    "\n",
    "def extract_answer(response: str) -> str:\n",
    "    \"\"\"Extract the answer choice (A, B, C, or D) from the model's response.\n",
    "    \n",
    "    STRICT matching to avoid false positives from words like 'Compute' or 'evAluate'.\n",
    "    \"\"\"\n",
    "    # Clean the response\n",
    "    response = response.strip()\n",
    "    \n",
    "    # First, try exact patterns that indicate a clear answer\n",
    "    # Pattern 1: Single letter possibly with punctuation at START of response\n",
    "    # Examples: \"A\", \"A.\", \"A)\", \"A:\", \"B \" etc.\n",
    "    match = re.match(r'^([A-Da-d])[\\.\\)\\:\\s]?', response)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    \n",
    "    # Pattern 2: \"Answer: A\" or \"The answer is B\" etc.\n",
    "    match = re.search(r'(?:answer|choice)(?:\\s+is)?[\\s:]+([A-Da-d])(?:[^a-zA-Z]|$)', response, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    \n",
    "    # Pattern 3: Single letter answer on its own line\n",
    "    for line in response.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if re.match(r'^([A-Da-d])[\\.\\)\\:]?$', line):\n",
    "            return line[0].upper()\n",
    "    \n",
    "    # Pattern 4: Response is ONLY a single letter (after stripping whitespace)\n",
    "    if len(response) == 1 and response.upper() in ['A', 'B', 'C', 'D']:\n",
    "        return response.upper()\n",
    "    \n",
    "    # If none of the strict patterns match, return UNKNOWN\n",
    "    # Do NOT try to find letters within words!\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def format_mmlu_prompt(question: str, choices: List[str]) -> str:\n",
    "    \"\"\"Format a MMLU question with multiple choice options.\"\"\"\n",
    "    prompt = f\"\"\"Answer the following multiple choice question. Respond with only the letter (A, B, C, or D) of the correct answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "A) {choices[0]}\n",
    "B) {choices[1]}\n",
    "C) {choices[2]}\n",
    "D) {choices[3]}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def save_checkpoint(results_dict: Dict[int, Dict], checkpoint_file: Path):\n",
    "    \"\"\"Save checkpoint with question indices as keys.\"\"\"\n",
    "    checkpoint_data = {\n",
    "        'results': results_dict,  # Dict with int keys (JSON converts to strings)\n",
    "        'timestamp': time.time(),\n",
    "        'num_processed': len(results_dict)\n",
    "    }\n",
    "    \n",
    "    # Atomic save to prevent corruption\n",
    "    temp_file = checkpoint_file.with_suffix('.tmp')\n",
    "    with open(temp_file, 'w') as f:\n",
    "        json.dump(checkpoint_data, f, indent=2)\n",
    "    temp_file.replace(checkpoint_file)  # Atomic rename\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file: Path) -> Dict[int, Dict]:\n",
    "    \"\"\"Load checkpoint and return results_dict with int keys.\"\"\"\n",
    "    if not checkpoint_file.exists():\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "        \n",
    "        results_raw = checkpoint_data.get('results', {})\n",
    "        \n",
    "        # Convert JSON string keys back to int\n",
    "        results_dict = {int(k): v for k, v in results_raw.items()}\n",
    "        \n",
    "        return results_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to load checkpoint: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def process_single_question(server_config: Dict, item: Dict, idx: int, timeout: int) -> Dict:\n",
    "    \"\"\"Process a single question using the Ollama API. Returns None on failure.\"\"\"\n",
    "    try:\n",
    "        prompt = format_mmlu_prompt(item['question'], item['choices'])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = generate_response(server_config, prompt, timeout)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        predicted_answer = extract_answer(response)\n",
    "        correct_answer = chr(65 + item['answer'])\n",
    "        \n",
    "        return {\n",
    "            'subject': item['subject'],\n",
    "            'question': item['question'],\n",
    "            'choices': item['choices'],\n",
    "            'correct_answer': correct_answer,\n",
    "            'predicted_answer': predicted_answer,\n",
    "            'full_response': response,\n",
    "            'is_correct': predicted_answer == correct_answer,\n",
    "            'time_seconds': elapsed_time\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Return None to trigger retry on different server\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking API servers...\n",
      "✓ Server 1: healthy - Model: llama3:8b-instruct-q4_K_M - URL: http://127.0.0.1:11434\n",
      "\n",
      "1 server(s) ready for benchmark\n"
     ]
    }
   ],
   "source": [
    "# Check API health for all servers\n",
    "print(\"Checking API servers...\")\n",
    "available_servers = []\n",
    "\n",
    "for i, server_config in enumerate(API_SERVERS):\n",
    "    try:\n",
    "        health = check_api_health(server_config)\n",
    "        print(f\"✓ Server {i+1}: {health['status']} - Model: {health['model']} - URL: {health['base_url']}\")\n",
    "        available_servers.append(server_config)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Server {i+1} ({server_config['base_url']}): UNAVAILABLE - {e}\")\n",
    "\n",
    "if not available_servers:\n",
    "    raise RuntimeError(\"No servers available! Check your server URLs and ensure servers are running.\")\n",
    "\n",
    "# Update global list to only available servers\n",
    "API_SERVERS = available_servers\n",
    "print(f\"\\n{len(API_SERVERS)} server(s) ready for benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "================================================================================\n",
      "  - llama3:8b-instruct-q4_K_M\n",
      "  - llama3:8b-instruct-q4_0\n",
      "  - olmo-3:32b\n",
      "  - olmo-3:7b\n",
      "  - nemotron-3-nano:30b\n",
      "  - ministral-3:8b\n",
      "  - ministral-3:3b\n",
      "  - ministral-3:14b\n",
      "\n",
      "================================================================================\n",
      "Configured model in notebook: llama3:8b-instruct-q4_K_M\n",
      "✓ Model 'llama3:8b-instruct-q4_K_M' is available\n"
     ]
    }
   ],
   "source": [
    "# Check available models first\n",
    "try:\n",
    "    tags_response = requests.get(f\"{API_BASE_URL}/api/tags\", timeout=10)\n",
    "    tags_response.raise_for_status()\n",
    "    models_data = tags_response.json()\n",
    "    \n",
    "    print(\"Available models:\")\n",
    "    print(\"=\" * 80)\n",
    "    if 'models' in models_data:\n",
    "        for model in models_data['models']:\n",
    "            print(f\"  - {model.get('name', 'unknown')}\")\n",
    "    else:\n",
    "        print(\"No models found or unexpected response format\")\n",
    "        print(f\"Response: {models_data}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Configured model in notebook: {MODEL_NAME}\")\n",
    "    \n",
    "    # Check if configured model exists\n",
    "    if 'models' in models_data:\n",
    "        model_names = [m.get('name', '') for m in models_data['models']]\n",
    "        if MODEL_NAME in model_names:\n",
    "            print(f\"✓ Model '{MODEL_NAME}' is available\")\n",
    "        else:\n",
    "            print(f\"✗ Model '{MODEL_NAME}' NOT FOUND!\")\n",
    "            print(f\"\\nYou may need to update MODEL_NAME in cell 4 to one of the available models above.\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error checking models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing request to: http://127.0.0.1:11434/api/chat\n",
      "Model: llama3:8b-instruct-q4_K_M\n",
      "\n",
      "Request payload:\n",
      "{\n",
      "  \"model\": \"llama3:8b-instruct-q4_K_M\",\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Say only the letter A.\"\n",
      "    }\n",
      "  ],\n",
      "  \"stream\": false,\n",
      "  \"options\": {\n",
      "    \"temperature\": 0.0,\n",
      "    \"num_predict\": 5\n",
      "  }\n",
      "}\n",
      "\n",
      "Response status: 200\n",
      "Response headers: {'Content-Type': 'application/json; charset=utf-8', 'Date': 'Fri, 09 Jan 2026 01:33:19 GMT', 'Content-Length': '311'}\n",
      "\n",
      "Response structure: ['model', 'created_at', 'message', 'done', 'done_reason', 'total_duration', 'load_duration', 'prompt_eval_count', 'prompt_eval_duration', 'eval_count', 'eval_duration']\n",
      "Full response:\n",
      "{\n",
      "  \"model\": \"llama3:8b-instruct-q4_K_M\",\n",
      "  \"created_at\": \"2026-01-09T01:33:19.1709428Z\",\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"A\"\n",
      "  },\n",
      "  \"done\": true,\n",
      "  \"done_reason\": \"stop\",\n",
      "  \"total_duration\": 16706875500,\n",
      "  \"load_duration\": 16611528300,\n",
      "  \"prompt_eval_count\": 16,\n",
      "  \"prompt_eval_duration\": 57402000,\n",
      "  \"eval_count\": 2,\n",
      "  \"eval_duration\": 30804900\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Test the exact request format\n",
    "import requests\n",
    "import json\n",
    "\n",
    "test_url = f\"{API_BASE_URL}/api/chat\"\n",
    "test_payload = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say only the letter A.\"\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "    \"options\": {\n",
    "        \"temperature\": 0.0,\n",
    "        \"num_predict\": 5\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Testing request to: {test_url}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"\\nRequest payload:\")\n",
    "print(json.dumps(test_payload, indent=2))\n",
    "\n",
    "try:\n",
    "    response = requests.post(test_url, json=test_payload, timeout=30)\n",
    "    print(f\"\\nResponse status: {response.status_code}\")\n",
    "    print(f\"Response headers: {dict(response.headers)}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"\\nResponse structure: {list(result.keys())}\")\n",
    "        print(f\"Full response:\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "    else:\n",
    "        print(f\"\\nError response:\")\n",
    "        print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load MMLU Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MMLU dataset from Hugging Face...\n",
      "This may take a few minutes on first run...\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Total questions: 14042\n",
      "Number of subjects: 57\n",
      "\n",
      "Subjects: abstract_algebra, anatomy, astronomy, business_ethics, clinical_knowledge, college_biology, college_chemistry, college_computer_science, college_mathematics, college_medicine...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading MMLU dataset from Hugging Face...\")\n",
    "print(\"This may take a few minutes on first run...\")\n",
    "\n",
    "# Load the MMLU dataset (using the 'test' split)\n",
    "# The dataset is organized by subjects\n",
    "dataset = load_dataset(\"cais/mmlu\", \"all\", split=\"test\")\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Total questions: {len(dataset)}\")\n",
    "\n",
    "# Get all unique subjects\n",
    "all_subjects = sorted(set(dataset['subject']))\n",
    "print(f\"Number of subjects: {len(all_subjects)}\")\n",
    "print(f\"\\nSubjects: {', '.join(all_subjects[:10])}...\" if len(all_subjects) > 10 else f\"\\nSubjects: {', '.join(all_subjects)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 14042 test questions across 57 subjects\n",
      "\n",
      "Example question:\n",
      "Subject: abstract_algebra\n",
      "Question: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n",
      "Choices: ['0', '4', '2', '6']\n",
      "Correct Answer: B\n"
     ]
    }
   ],
   "source": [
    "# Filter subjects if specified\n",
    "subjects_to_test = SELECTED_SUBJECTS if SELECTED_SUBJECTS else all_subjects\n",
    "\n",
    "# Prepare test questions\n",
    "test_questions = []\n",
    "\n",
    "for subject in subjects_to_test:\n",
    "    # Filter dataset by subject\n",
    "    subject_data = [item for item in dataset if item['subject'] == subject]\n",
    "    \n",
    "    # Limit number of questions if specified\n",
    "    if NUM_QUESTIONS_PER_SUBJECT:\n",
    "        subject_data = subject_data[:NUM_QUESTIONS_PER_SUBJECT]\n",
    "    \n",
    "    test_questions.extend(subject_data)\n",
    "\n",
    "print(f\"Prepared {len(test_questions)} test questions across {len(subjects_to_test)} subjects\")\n",
    "print(f\"\\nExample question:\")\n",
    "example = test_questions[0]\n",
    "print(f\"Subject: {example['subject']}\")\n",
    "print(f\"Question: {example['question']}\")\n",
    "print(f\"Choices: {example['choices']}\")\n",
    "print(f\"Correct Answer: {chr(65 + example['answer'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting benchmark:\n",
      "  Total questions: 14042\n",
      "  Already completed: 0\n",
      "  Remaining: 14042\n",
      "  Servers: 1\n",
      "================================================================================\n",
      "\n",
      "Starting 1 worker thread(s)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:   4%|▎         | 501/14042 [01:41<46:02,  4.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q500/14042] Progress Update:\n",
      "  Current Accuracy: 60.2% (301/500)\n",
      "  Latest: ✓ clinical_knowledge - Answer: B (Correct: B)\n",
      "  Avg Time: 0.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:   7%|▋         | 1001/14042 [03:22<47:43,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q1000/14042] Progress Update:\n",
      "  Current Accuracy: 62.5% (625/1000)\n",
      "  Latest: ✓ college_computer_science - Answer: A (Correct: A)\n",
      "  Avg Time: 0.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  11%|█         | 1500/14042 [05:06<47:15,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q1500/14042] Progress Update:\n",
      "  Current Accuracy: 57.7% (865/1500)\n",
      "  Latest: ✓ computer_security - Answer: C (Correct: C)\n",
      "  Avg Time: 0.22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  14%|█▍        | 2001/14042 [06:47<45:22,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q2000/14042] Progress Update:\n",
      "  Current Accuracy: 57.0% (1139/2000)\n",
      "  Latest: ✗ electrical_engineering - Answer: B (Correct: C)\n",
      "  Avg Time: 0.24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  18%|█▊        | 2501/14042 [08:28<41:37,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q2500/14042] Progress Update:\n",
      "  Current Accuracy: 55.1% (1378/2500)\n",
      "  Latest: ✗ formal_logic - Answer: UNKNOWN (Correct: B)\n",
      "  Avg Time: 0.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  21%|██▏       | 3001/14042 [10:10<38:51,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q3000/14042] Progress Update:\n",
      "  Current Accuracy: 56.3% (1688/3000)\n",
      "  Latest: ✗ high_school_chemistry - Answer: A (Correct: B)\n",
      "  Avg Time: 0.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  25%|██▍       | 3501/14042 [12:04<39:04,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q3500/14042] Progress Update:\n",
      "  Current Accuracy: 57.1% (1997/3500)\n",
      "  Latest: ✗ high_school_geography - Answer: A (Correct: B)\n",
      "  Avg Time: 0.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  28%|██▊       | 4001/14042 [13:47<37:55,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q4000/14042] Progress Update:\n",
      "  Current Accuracy: 59.3% (2373/4000)\n",
      "  Latest: ✓ high_school_macroeconomics - Answer: B (Correct: B)\n",
      "  Avg Time: 0.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  32%|███▏      | 4501/14042 [15:31<39:09,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q4500/14042] Progress Update:\n",
      "  Current Accuracy: 57.1% (2568/4500)\n",
      "  Latest: ✗ high_school_microeconomics - Answer: B (Correct: C)\n",
      "  Avg Time: 0.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  36%|███▌      | 5001/14042 [17:15<34:28,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q5000/14042] Progress Update:\n",
      "  Current Accuracy: 57.6% (2882/5000)\n",
      "  Latest: ✓ high_school_psychology - Answer: C (Correct: C)\n",
      "  Avg Time: 0.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  39%|███▉      | 5501/14042 [18:57<33:33,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q5500/14042] Progress Update:\n",
      "  Current Accuracy: 59.2% (3255/5500)\n",
      "  Latest: ✗ high_school_statistics - Answer: A (Correct: B)\n",
      "  Avg Time: 0.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  43%|████▎     | 6000/14042 [20:59<39:46,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q6000/14042] Progress Update:\n",
      "  Current Accuracy: 59.8% (3586/6000)\n",
      "  Latest: ✓ high_school_world_history - Answer: C (Correct: C)\n",
      "  Avg Time: 0.22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  46%|████▋     | 6501/14042 [22:50<29:47,  4.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q6500/14042] Progress Update:\n",
      "  Current Accuracy: 60.8% (3953/6500)\n",
      "  Latest: ✗ international_law - Answer: C (Correct: B)\n",
      "  Avg Time: 0.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  50%|████▉     | 7000/14042 [24:31<32:09,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q7000/14042] Progress Update:\n",
      "  Current Accuracy: 61.2% (4287/7000)\n",
      "  Latest: ✗ management - Answer: B (Correct: C)\n",
      "  Avg Time: 0.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  53%|█████▎    | 7500/14042 [26:28<50:36,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q7500/14042] Progress Update:\n",
      "  Current Accuracy: 62.5% (4688/7500)\n",
      "  Latest: ✓ miscellaneous - Answer: A (Correct: A)\n",
      "  Avg Time: 0.31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  57%|█████▋    | 8000/14042 [28:12<29:42,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q8000/14042] Progress Update:\n",
      "  Current Accuracy: 63.7% (5099/8000)\n",
      "  Latest: ✗ miscellaneous - Answer: A (Correct: B)\n",
      "  Avg Time: 0.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  61%|██████    | 8501/14042 [29:54<23:33,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q8500/14042] Progress Update:\n",
      "  Current Accuracy: 64.0% (5440/8500)\n",
      "  Latest: ✗ moral_disputes - Answer: C (Correct: B)\n",
      "  Avg Time: 0.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  64%|██████▍   | 9000/14042 [31:39<23:53,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q9000/14042] Progress Update:\n",
      "  Current Accuracy: 61.9% (5568/9000)\n",
      "  Latest: ✗ moral_scenarios - Answer: B (Correct: D)\n",
      "  Avg Time: 0.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  68%|██████▊   | 9500/14042 [33:23<22:47,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q9500/14042] Progress Update:\n",
      "  Current Accuracy: 60.6% (5755/9500)\n",
      "  Latest: ✓ nutrition - Answer: A (Correct: A)\n",
      "  Avg Time: 0.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  71%|███████   | 10001/14042 [35:06<17:17,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q10000/14042] Progress Update:\n",
      "  Current Accuracy: 61.0% (6100/10000)\n",
      "  Latest: ✗ philosophy - Answer: B (Correct: A)\n",
      "  Avg Time: 0.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  75%|███████▍  | 10500/14042 [37:07<18:33,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q10500/14042] Progress Update:\n",
      "  Current Accuracy: 61.1% (6420/10500)\n",
      "  Latest: ✗ professional_accounting - Answer: B (Correct: A)\n",
      "  Avg Time: 0.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  78%|███████▊  | 11000/14042 [39:07<17:05,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q11000/14042] Progress Update:\n",
      "  Current Accuracy: 60.4% (6639/11000)\n",
      "  Latest: ✗ professional_law - Answer: A (Correct: C)\n",
      "  Avg Time: 0.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  82%|████████▏ | 11500/14042 [41:09<14:35,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q11500/14042] Progress Update:\n",
      "  Current Accuracy: 59.5% (6840/11500)\n",
      "  Latest: ✗ professional_law - Answer: A (Correct: C)\n",
      "  Avg Time: 0.22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  85%|████████▌ | 12000/14042 [43:11<11:44,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q12000/14042] Progress Update:\n",
      "  Current Accuracy: 58.7% (7043/12000)\n",
      "  Latest: ✓ professional_law - Answer: B (Correct: B)\n",
      "  Avg Time: 0.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  89%|████████▉ | 12500/14042 [45:06<08:31,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q12500/14042] Progress Update:\n",
      "  Current Accuracy: 58.9% (7360/12500)\n",
      "  Latest: ✗ professional_psychology - Answer: A (Correct: B)\n",
      "  Avg Time: 0.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  93%|█████████▎| 13000/14042 [46:50<05:35,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q13000/14042] Progress Update:\n",
      "  Current Accuracy: 59.1% (7683/13000)\n",
      "  Latest: ✓ professional_psychology - Answer: A (Correct: A)\n",
      "  Avg Time: 0.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark:  96%|█████████▌| 13501/14042 [48:37<02:40,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q13500/14042] Progress Update:\n",
      "  Current Accuracy: 59.4% (8015/13500)\n",
      "  Latest: ✓ sociology - Answer: A (Correct: A)\n",
      "  Avg Time: 0.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark: 100%|█████████▉| 14000/14042 [50:18<00:13,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q14000/14042] Progress Update:\n",
      "  Current Accuracy: 59.8% (8365/14000)\n",
      "  Latest: ✓ world_religions - Answer: C (Correct: C)\n",
      "  Avg Time: 0.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MMLU Benchmark: 100%|██████████| 14042/14042 [50:28<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Benchmark completed!\n",
      "Total: 14042 | Correct: 8405 | Accuracy: 59.86%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Sanitize model name for use in filenames (replace : and / with -)\n",
    "model_name_safe = MODEL_NAME.replace(':', '-').replace('/', '-')\n",
    "\n",
    "# Load checkpoint\n",
    "run_timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_file = RESULTS_DIR / f\"checkpoint_ollama_{model_name_safe}_{run_timestamp}.json\"\n",
    "results_dict = {}\n",
    "\n",
    "if ENABLE_CHECKPOINTS:\n",
    "    checkpoint_files = sorted(RESULTS_DIR.glob(f\"checkpoint_ollama_{model_name_safe}_*.json\"), reverse=True)\n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = checkpoint_files[0]\n",
    "        results_dict = load_checkpoint(latest_checkpoint)\n",
    "        checkpoint_file = latest_checkpoint\n",
    "        print(f\"Found checkpoint: {latest_checkpoint.name}\")\n",
    "        print(f\"Loaded {len(results_dict)} completed questions from checkpoint\")\n",
    "\n",
    "# Calculate subject_stats from loaded results (always recalculate, don't trust checkpoint)\n",
    "subject_stats = {}\n",
    "for result in results_dict.values():\n",
    "    subject = result['subject']\n",
    "    if subject not in subject_stats:\n",
    "        subject_stats[subject] = {'correct': 0, 'total': 0, 'time': 0}\n",
    "    subject_stats[subject]['total'] += 1\n",
    "    subject_stats[subject]['time'] += result['time_seconds']\n",
    "    if result['is_correct']:\n",
    "        subject_stats[subject]['correct'] += 1\n",
    "\n",
    "# Initialize shared state\n",
    "state_lock = threading.Lock()              # Single lock for simplicity\n",
    "question_queue = queue.Queue()             # Thread-safe by default\n",
    "progress_queue = queue.Queue()             # For progress updates\n",
    "\n",
    "# Fill queue with unprocessed questions\n",
    "for idx in range(len(test_questions)):\n",
    "    if idx not in results_dict:\n",
    "        question_queue.put(idx)\n",
    "\n",
    "remaining_count = question_queue.qsize()\n",
    "\n",
    "print(f\"\\nStarting benchmark:\")\n",
    "print(f\"  Total questions: {len(test_questions)}\")\n",
    "print(f\"  Already completed: {len(results_dict)}\")\n",
    "print(f\"  Remaining: {remaining_count}\")\n",
    "print(f\"  Servers: {len(API_SERVERS)}\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Worker function (runs on each thread - one per server)\n",
    "def worker(server_config, worker_id):\n",
    "    retry_delay = 5  # Exponential backoff for failures\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            idx = question_queue.get(timeout=1)\n",
    "        except queue.Empty:\n",
    "            return  # Exit when queue empty\n",
    "\n",
    "        # CRITICAL: Check if already processed (race condition protection)\n",
    "        with state_lock:\n",
    "            if idx in results_dict:\n",
    "                continue  # Another worker got it first, skip\n",
    "\n",
    "        # Process question (no lock held during long operation!)\n",
    "        result = process_single_question(\n",
    "            server_config,\n",
    "            test_questions[idx],\n",
    "            idx,\n",
    "            TIMEOUT_SECONDS\n",
    "        )\n",
    "\n",
    "        if result is not None:\n",
    "            # Success - update all shared state atomically\n",
    "            with state_lock:\n",
    "                results_dict[idx] = result\n",
    "\n",
    "                # Update subject stats\n",
    "                subject = result['subject']\n",
    "                if subject not in subject_stats:\n",
    "                    subject_stats[subject] = {'correct': 0, 'total': 0, 'time': 0}\n",
    "                subject_stats[subject]['total'] += 1\n",
    "                subject_stats[subject]['time'] += result['time_seconds']\n",
    "                if result['is_correct']:\n",
    "                    subject_stats[subject]['correct'] += 1\n",
    "\n",
    "                # Save checkpoint periodically\n",
    "                if ENABLE_CHECKPOINTS and len(results_dict) % CHECKPOINT_INTERVAL == 0:\n",
    "                    save_checkpoint(results_dict, checkpoint_file)\n",
    "\n",
    "            # Send progress update (outside lock)\n",
    "            progress_queue.put(('success', idx, result, worker_id))\n",
    "            retry_delay = 5  # Reset delay on success\n",
    "\n",
    "        else:\n",
    "            # Failed - re-queue for retry on different server\n",
    "            question_queue.put(idx)\n",
    "            progress_queue.put(('retry', idx, server_config['base_url'], worker_id))\n",
    "\n",
    "            # Exponential backoff with jitter to avoid thundering herd\n",
    "            time.sleep(min(retry_delay, 60) + random.uniform(0, 2))\n",
    "            retry_delay *= 1.5\n",
    "\n",
    "# Progress display in separate thread (avoids tqdm threading issues)\n",
    "# MINIMAL OUTPUT: Only print every 500 questions to prevent kernel death\n",
    "PRINT_INTERVAL = 500  # Print detailed output every N questions\n",
    "\n",
    "def display_progress():\n",
    "    processed_count = len(results_dict)\n",
    "    correct_count = sum(1 for r in results_dict.values() if r['is_correct'])\n",
    "\n",
    "    with tqdm(total=len(test_questions), initial=processed_count, desc=\"MMLU Benchmark\") as pbar:\n",
    "        while True:\n",
    "            try:\n",
    "                event = progress_queue.get(timeout=0.1)\n",
    "                event_type = event[0]\n",
    "\n",
    "                if event_type == 'success':\n",
    "                    _, idx, result, worker_id = event\n",
    "                    pbar.update(1)\n",
    "                    processed_count += 1\n",
    "                    if result['is_correct']:\n",
    "                        correct_count += 1\n",
    "\n",
    "                    accuracy = (correct_count / processed_count) * 100\n",
    "                    status = \"✓\" if result['is_correct'] else \"✗\"\n",
    "\n",
    "                    # Only print detailed info every PRINT_INTERVAL questions\n",
    "                    if processed_count % PRINT_INTERVAL == 0:\n",
    "                        print(f\"\\n[Q{processed_count}/{len(test_questions)}] Progress Update:\")\n",
    "                        print(f\"  Current Accuracy: {accuracy:.1f}% ({correct_count}/{processed_count})\")\n",
    "                        print(f\"  Latest: {status} {result['subject']} - Answer: {result['predicted_answer']} (Correct: {result['correct_answer']})\")\n",
    "                        print(f\"  Avg Time: {result['time_seconds']:.2f}s\")\n",
    "\n",
    "                elif event_type == 'retry':\n",
    "                    _, idx, server_url, worker_id = event\n",
    "                    print(f\"\\n[Q{idx+1}] ⚠ Retrying on different server (failed on Server {worker_id+1})\")\n",
    "\n",
    "                elif event_type == 'done':\n",
    "                    break\n",
    "\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "\n",
    "# Start progress display thread\n",
    "progress_thread = threading.Thread(target=display_progress, daemon=True)\n",
    "progress_thread.start()\n",
    "\n",
    "# Launch worker threads (one per server)\n",
    "print(f\"Starting {len(API_SERVERS)} worker thread(s)...\\n\")\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(API_SERVERS)) as executor:\n",
    "    futures = [\n",
    "        executor.submit(worker, server_config, i)\n",
    "        for i, server_config in enumerate(API_SERVERS)\n",
    "    ]\n",
    "\n",
    "    # Wait for all workers to complete\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "# Signal progress thread to stop\n",
    "progress_queue.put(('done',))\n",
    "progress_thread.join(timeout=1)\n",
    "\n",
    "# Final checkpoint save\n",
    "if ENABLE_CHECKPOINTS:\n",
    "    save_checkpoint(results_dict, checkpoint_file)\n",
    "\n",
    "# Calculate final stats\n",
    "total_questions = len(results_dict)\n",
    "correct_count = sum(1 for r in results_dict.values() if r['is_correct'])\n",
    "final_accuracy = (correct_count / total_questions) * 100 if total_questions > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Benchmark completed!\")\n",
    "print(f\"Total: {total_questions} | Correct: {correct_count} | Accuracy: {final_accuracy:.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calculate and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OVERALL RESULTS\n",
      "================================================================================\n",
      "Total Questions: 14042\n",
      "Correct Answers: 8405\n",
      "Incorrect Answers: 5637\n",
      "Overall Accuracy: 59.86%\n",
      "Total Time: 3020.16 seconds\n",
      "Average Time per Question: 0.22 seconds\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SUBJECT-WISE RESULTS\n",
      "================================================================================\n",
      "                            Subject  Correct  Total Accuracy (%) Avg Time (s)\n",
      "                   abstract_algebra       29    100        29.00         0.21\n",
      "                            anatomy       93    135        68.89         0.20\n",
      "                          astronomy      102    152        67.11         0.20\n",
      "                    business_ethics       67    100        67.00         0.21\n",
      "                 clinical_knowledge      187    265        70.57         0.20\n",
      "                    college_biology      104    144        72.22         0.20\n",
      "                  college_chemistry       40    100        40.00         0.21\n",
      "           college_computer_science       42    100        42.00         0.21\n",
      "                college_mathematics       30    100        30.00         0.21\n",
      "                   college_medicine      106    173        61.27         0.21\n",
      "                    college_physics       45    102        44.12         0.21\n",
      "                  computer_security       70    100        70.00         0.20\n",
      "                 conceptual_physics      129    235        54.89         0.20\n",
      "                       econometrics       48    114        42.11         0.21\n",
      "             electrical_engineering       85    145        58.62         0.20\n",
      "             elementary_mathematics      175    378        46.30         0.20\n",
      "                       formal_logic       57    126        45.24         0.21\n",
      "                       global_facts       38    100        38.00         0.20\n",
      "                high_school_biology      232    310        74.84         0.20\n",
      "              high_school_chemistry       94    203        46.31         0.21\n",
      "       high_school_computer_science       60    100        60.00         0.21\n",
      "       high_school_european_history      121    165        73.33         0.26\n",
      "              high_school_geography      158    198        79.80         0.20\n",
      "high_school_government_and_politics      156    193        80.83         0.20\n",
      "         high_school_macroeconomics      229    390        58.72         0.21\n",
      "            high_school_mathematics       71    270        26.30         0.21\n",
      "         high_school_microeconomics      164    238        68.91         0.21\n",
      "                high_school_physics       54    151        35.76         0.21\n",
      "             high_school_psychology      443    545        81.28         0.20\n",
      "             high_school_statistics       95    216        43.98         0.21\n",
      "             high_school_us_history      150    204        73.53         0.26\n",
      "          high_school_world_history      186    237        78.48         0.28\n",
      "                        human_aging      150    223        67.26         0.20\n",
      "                    human_sexuality      101    131        77.10         0.20\n",
      "                  international_law       84    121        69.42         0.20\n",
      "                      jurisprudence       75    108        69.44         0.20\n",
      "                  logical_fallacies      126    163        77.30         0.20\n",
      "                   machine_learning       48    112        42.86         0.20\n",
      "                         management       79    103        76.70         0.20\n",
      "                          marketing      201    234        85.90         0.20\n",
      "                   medical_genetics       75    100        75.00         0.20\n",
      "                      miscellaneous      635    783        81.10         0.22\n",
      "                     moral_disputes      215    346        62.14         0.20\n",
      "                    moral_scenarios      238    895        26.59         0.21\n",
      "                          nutrition      216    306        70.59         0.20\n",
      "                         philosophy      210    311        67.52         0.21\n",
      "                         prehistory      235    324        72.53         0.22\n",
      "            professional_accounting      135    282        47.87         0.26\n",
      "                   professional_law      641   1534        41.79         0.24\n",
      "              professional_medicine      193    272        70.96         0.22\n",
      "            professional_psychology      396    612        64.71         0.21\n",
      "                   public_relations       69    110        62.73         0.20\n",
      "                   security_studies      157    245        64.08         0.22\n",
      "                          sociology      163    201        81.09         0.21\n",
      "                  us_foreign_policy       85    100        85.00         0.21\n",
      "                           virology       81    166        48.80         0.20\n",
      "                    world_religions      137    171        80.12         0.20\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Convert results_dict to list for analysis\n",
    "results = list(results_dict.values())\n",
    "\n",
    "# Overall statistics\n",
    "total_questions = len(results)\n",
    "correct_answers = sum(1 for r in results if r['is_correct'])\n",
    "overall_accuracy = (correct_answers / total_questions) * 100 if total_questions > 0 else 0\n",
    "total_time = sum(r['time_seconds'] for r in results)\n",
    "avg_time = total_time / total_questions if total_questions > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total Questions: {total_questions}\")\n",
    "print(f\"Correct Answers: {correct_answers}\")\n",
    "print(f\"Incorrect Answers: {total_questions - correct_answers}\")\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.2f}%\")\n",
    "print(f\"Total Time: {total_time:.2f} seconds\")\n",
    "print(f\"Average Time per Question: {avg_time:.2f} seconds\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Subject-wise results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUBJECT-WISE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "subject_results = []\n",
    "for subject, stats in sorted(subject_stats.items()):\n",
    "    accuracy = (stats['correct'] / stats['total']) * 100 if stats['total'] > 0 else 0\n",
    "    avg_time_subject = stats['time'] / stats['total'] if stats['total'] > 0 else 0\n",
    "    subject_results.append({\n",
    "        'Subject': subject,\n",
    "        'Correct': stats['correct'],\n",
    "        'Total': stats['total'],\n",
    "        'Accuracy (%)': f\"{accuracy:.2f}\",\n",
    "        'Avg Time (s)': f\"{avg_time_subject:.2f}\"\n",
    "    })\n",
    "\n",
    "df_subjects = pd.DataFrame(subject_results)\n",
    "print(df_subjects.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE INCORRECT ANSWERS (showing up to 5 of 5637)\n",
      "================================================================================\n",
      "\n",
      "[1] Subject: abstract_algebra\n",
      "Question: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n",
      "Choices:\n",
      "  A) 0\n",
      "  B) 4\n",
      "  C) 2\n",
      "  D) 6\n",
      "Correct Answer: B\n",
      "Model Answer: C\n",
      "Full Response: C\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] Subject: abstract_algebra\n",
      "Question: Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the index of <p> in S_5.\n",
      "Choices:\n",
      "  A) 8\n",
      "  B) 2\n",
      "  C) 24\n",
      "  D) 120\n",
      "Correct Answer: C\n",
      "Model Answer: B\n",
      "Full Response: B\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] Subject: abstract_algebra\n",
      "Question: Statement 1 | A factor group of a non-Abelian group is non-Abelian. Statement 2 | If K is a normal subgroup of H and H is a normal subgroup of G, then K is a normal subgroup of G.\n",
      "Choices:\n",
      "  A) True, True\n",
      "  B) False, False\n",
      "  C) True, False\n",
      "  D) False, True\n",
      "Correct Answer: B\n",
      "Model Answer: UNKNOWN\n",
      "Full Response: The correct answer\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4] Subject: abstract_algebra\n",
      "Question: Statement 1 | If a group has an element of order 15 it must have at least 8 elements of order 15. Statement 2 | If a group has more than 8 elements of order 15, it must have at least 16 elements of order 15.\n",
      "Choices:\n",
      "  A) True, True\n",
      "  B) False, False\n",
      "  C) True, False\n",
      "  D) False, True\n",
      "Correct Answer: A\n",
      "Model Answer: C\n",
      "Full Response: C) True\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[5] Subject: abstract_algebra\n",
      "Question: Statement 1 | Every homomorphic image of a group G is isomorphic to a factor group of G. Statement 2 | The homomorphic images of a group G are the same (up to isomorphism) as the factor groups of G.\n",
      "Choices:\n",
      "  A) True, True\n",
      "  B) False, False\n",
      "  C) True, False\n",
      "  D) False, True\n",
      "Correct Answer: A\n",
      "Model Answer: UNKNOWN\n",
      "Full Response: The correct answer\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show some incorrect answers for analysis\n",
    "# Convert results_dict to list if not already done\n",
    "if isinstance(results_dict, dict):\n",
    "    results = list(results_dict.values())\n",
    "\n",
    "incorrect_results = [r for r in results if not r['is_correct']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SAMPLE INCORRECT ANSWERS (showing up to 5 of {len(incorrect_results)})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(incorrect_results[:5]):\n",
    "    print(f\"\\n[{i+1}] Subject: {result['subject']}\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Choices:\")\n",
    "    for idx, choice in enumerate(result['choices']):\n",
    "        print(f\"  {chr(65+idx)}) {choice}\")\n",
    "    print(f\"Correct Answer: {result['correct_answer']}\")\n",
    "    print(f\"Model Answer: {result['predicted_answer']}\")\n",
    "    print(f\"Full Response: {result['full_response'][:200]}...\" if len(result['full_response']) > 200 else f\"Full Response: {result['full_response']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed results saved to: results\\mmlu_ollama_llama3-8b-instruct-q4_K_M_results_20260109_032446.csv\n",
      "Summary saved to: results\\mmlu_ollama_llama3-8b-instruct-q4_K_M_summary_20260109_032446.txt\n",
      "Checkpoint marked as completed: completed_checkpoint_ollama_llama3-8b-instruct-q4_K_M_20260109_023418.json\n",
      "\n",
      "Benchmark complete! All results saved to the 'results' folder.\n",
      "Results directory: c:\\Users\\user\\NextCloud\\nextcloud.nicojoerger.de\\Documents\\Schulen\\Studium\\Abschlussarbeit\\Git\\Embedded-CPU-LLM\\Code\\Notebooks\\results\n"
     ]
    }
   ],
   "source": [
    "# Convert results_dict to list if not already done\n",
    "if isinstance(results_dict, dict):\n",
    "    results = list(results_dict.values())\n",
    "\n",
    "# Create results dataframe\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Use the same timestamp from the benchmark run\n",
    "final_timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = RESULTS_DIR / f\"mmlu_ollama_{model_name_safe}_results_{final_timestamp}.csv\"\n",
    "summary_file = RESULTS_DIR / f\"mmlu_ollama_{model_name_safe}_summary_{final_timestamp}.txt\"\n",
    "\n",
    "# Save detailed results\n",
    "df_results.to_csv(results_file, index=False)\n",
    "print(f\"Detailed results saved to: {results_file}\")\n",
    "\n",
    "# Save summary\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"MMLU Benchmark Summary (Ollama)\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    f.write(f\"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"API Type: Ollama\\n\")\n",
    "    f.write(f\"Base URL: {API_BASE_URL}\\n\")\n",
    "    f.write(f\"Model: {MODEL_NAME}\\n\")\n",
    "    f.write(f\"Number of Servers: {len(API_SERVERS)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Configuration:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Questions per subject: {NUM_QUESTIONS_PER_SUBJECT if NUM_QUESTIONS_PER_SUBJECT else 'ALL'}\\n\")\n",
    "    f.write(f\"Timeout: {TIMEOUT_SECONDS}s\\n\")\n",
    "    f.write(f\"Temperature: {GENERATION_SETTINGS.get('temperature', 'N/A')}\\n\")\n",
    "    f.write(f\"Num Predict: {GENERATION_SETTINGS.get('num_predict', 'N/A')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Overall Results:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Total Questions: {total_questions}\\n\")\n",
    "    f.write(f\"Correct Answers: {correct_answers}\\n\")\n",
    "    f.write(f\"Accuracy: {overall_accuracy:.2f}%\\n\")\n",
    "    f.write(f\"Average Time: {avg_time:.2f} seconds\\n\\n\")\n",
    "    \n",
    "    f.write(\"Subject-wise Results:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(df_subjects.to_string(index=False))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "print(f\"Summary saved to: {summary_file}\")\n",
    "\n",
    "# Clean up checkpoint file if benchmark completed successfully\n",
    "if ENABLE_CHECKPOINTS and checkpoint_file.exists():\n",
    "    # Rename checkpoint to indicate completion\n",
    "    completed_checkpoint = RESULTS_DIR / f\"completed_{checkpoint_file.name}\"\n",
    "    checkpoint_file.rename(completed_checkpoint)\n",
    "    print(f\"Checkpoint marked as completed: {completed_checkpoint.name}\")\n",
    "\n",
    "print(\"\\nBenchmark complete! All results saved to the 'results' folder.\")\n",
    "print(f\"Results directory: {RESULTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
